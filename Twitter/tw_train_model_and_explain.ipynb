{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @authors: Raj Vardhan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import pandas\n",
    "import sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "import pickle\n",
    "import operator\n",
    "from keras.models import Sequential\n",
    "\n",
    "### Load data and model\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'twitter'\n",
    "\n",
    "directory = \"../twitter_data_new/\"\n",
    "model_dir = directory + \"model/\"\n",
    "model_name = 'model_twitter.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region 1: This code region is to be run for training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region 1.1. Run this if you are starting from scratch. Move to 1.2 if data is already shuffled and scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load datasets\n",
    "# file name can be passed as parameter\n",
    "df = pandas.read_csv(directory + \"honeypot.csv\", header=None)\n",
    "\n",
    "no_of_col = df.columns.size\n",
    "print('no. of columns is {}'.format(no_of_col))\n",
    "\n",
    "ds = df.values\n",
    "orig_X = ds[:,1:no_of_col].astype(float)\n",
    "orig_Y = ds[:,0]\n",
    "\n",
    "indices = np.random.permutation(len(orig_X))\n",
    "X_unscaled = orig_X[indices]\n",
    "Y = orig_Y[indices]\n",
    "\n",
    "np.save(directory + 'X_unscaled_shuffled.npy',X_unscaled)\n",
    "np.save(directory + 'Y_shuffled.npy',Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Features #0 to #5 : \n",
    "0 - age \n",
    "1 - NumerOfFollowings \n",
    "2 - NumberOfFollowers \n",
    "3 - NumberOfTweets      [see description of feature #6 for difference]\n",
    "4 - LengthOfScreenName \n",
    "5 - LengthOfDescriptionInUserProfile\n",
    "\n",
    "Features #6 to #15: \n",
    "6 - num_tws         [smaller or equal than feature #3 as it depends on sampling period]\n",
    "7 - ratio_question\n",
    "8 - ratio_exclam\n",
    "9 - len_tws\n",
    "10 - speed_tws\n",
    "11 - num_url\n",
    "12 - ratio_url\n",
    "13 - num_at\n",
    "14 - ratio_at  [What's the ratio of tweets containing @; some samples have > 1; could be corrected]\n",
    "15 - num_RT\n",
    "\n",
    "Features #16 to #25: \n",
    "16 - ratio_RT\n",
    "17 - num_uniq_at\n",
    "18 - ratio_uniq_at\n",
    "19 - num_reply\n",
    "20 - ratio_reply\n",
    "21 - num_hash\n",
    "22 - ratio_hash\n",
    "23 - jacc_tw\n",
    "24 - jacc_url\n",
    "25 - compress_ratio\n",
    "\n",
    "Features #26-#27: \n",
    "26 - spam word ratio\n",
    "27 - FollowingChangeRatio\n",
    "\n",
    "There are total 6+10+10+2=28 features.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use previously shuffled data (don't shuffle every time)\n",
    "X_unscaled = np.load(directory + 'X_unscaled_shuffled.npy')\n",
    "Y = np.load(directory + 'Y_shuffled.npy')\n",
    "\n",
    "Y_categ = to_categorical(Y)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train, test split\n",
    "x_train_unscaled, x_test_unscaled, y_train, y_test = train_test_split(X_unscaled,Y_categ,test_size=0.20, random_state=42, shuffle=True)\n",
    "\n",
    "# We will use this later for preparing the scaler object\n",
    "np.save(directory + \"x_train_unscaled.npy\", x_train_unscaled)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "#sc = StandardScaler()\n",
    "sc = MinMaxScaler()\n",
    "x_train = sc.fit_transform(x_train_unscaled)\n",
    "x_test = sc.transform(x_test_unscaled)\n",
    "\n",
    "# save the scaled data\n",
    "np.save(directory+'x_train.npy',x_train)\n",
    "np.save(directory+'y_train.npy',y_train)\n",
    "\n",
    "np.save(directory+'x_test.npy',x_test)\n",
    "np.save(directory+'y_test.npy',y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region 1.2: Load previously created train and test data that is shuffled and scaled to be between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(directory + '/x_train.npy')\n",
    "x_test = np.load(directory + '/x_test.npy')\n",
    "y_train = np.load(directory + '/y_train.npy')\n",
    "y_test = np.load(directory + '/y_test.npy')\n",
    "\n",
    "# build DNNs\n",
    "DNNmodel = Sequential()\n",
    "DNNmodel.add(Dense(20, input_dim = no_of_col-1, kernel_initializer=\"uniform\",activation=\"relu\"))\n",
    "DNNmodel.add(Dropout(0.5))\n",
    "DNNmodel.add(Dense(20, kernel_initializer=\"uniform\", activation=\"relu\"))\n",
    "DNNmodel.add(Dropout(0.5))\n",
    "DNNmodel.add(Dense(2))\n",
    "DNNmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "#compile Model\n",
    "DNNmodel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "# 3-fold validations\n",
    "history=DNNmodel.fit(x_train, y_train,validation_split=0.20,\n",
    "          epochs=100, batch_size=256)\n",
    "\n",
    "#evaluation\n",
    "score = DNNmodel.evaluate(x_test, y_test, batch_size=256)\n",
    "print(\"Test Accuracy\",score)\n",
    "print(\"\\n%s: %.2f%%\" % (DNNmodel.metrics_names[1], score[1]*100))\n",
    "\n",
    "# evaluation through predict function\n",
    "\n",
    "y_pred = DNNmodel.predict(x_test)\n",
    "\n",
    "tp =0\n",
    "fp =0\n",
    "tn =0\n",
    "fn =0\n",
    "for i in range(len(y_pred)):\n",
    "    pred_class = np.argmax(y_pred[i])\n",
    "    true_class = np.argmax(y_test[i])\n",
    "    if(pred_class == true_class):\n",
    "        if true_class == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "    else:\n",
    "        if true_class == 1:\n",
    "            fn += 1\n",
    "        else:\n",
    "            fp += 1        \n",
    "    \n",
    "precision = tp*100/ (tp+fp)\n",
    "recall = tp*100/ (tp+fn)\n",
    "acc = (tp+tn)*100/(tp+tn+fp+fn)\n",
    "\n",
    "total_actual_positives = tp+fn\n",
    "total_actual_negatives = fp+tn\n",
    "\n",
    "print(\"Accuracy is {}%\".format(acc))\n",
    "print('tp: {} fp: {} tn: {} fn:{} '.format(tp,fp,tn,fn))\n",
    "print(\"precision: {}%   recall: {}%\".format(precision, recall))\n",
    "\n",
    "print(\"Total true +ves: {} Total true -ves: {}\".format(total_actual_positives, total_actual_negatives))\n",
    "\n",
    "# Save the model\n",
    "DNNmodel.save(model_dir + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---x----- Region 1 ends ------x------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region 2 begins: Here we will do model explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model (Starting point)\n",
    "DNNmodel = load_model(model_dir + model_name)\n",
    "\n",
    "W = DNNmodel.get_weights()\n",
    "W={'weights1': W[0], \n",
    "   'weights2': W[2],\n",
    "   'weights3': W[4],\n",
    "   'biases1' : W[1],\n",
    "   'biases2' : W[3],    \n",
    "   'biases3' : W[5]    \n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
